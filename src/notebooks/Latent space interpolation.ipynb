{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ee07dd-702a-4736-a88b-286bd9ee2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491e3218-b8f1-4f5c-b4fa-75beea51cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers.models.unets.unet_2d import UNet2DOutput\n",
    "from typing import Tuple, Union, Optional\n",
    "\n",
    "class Unet(UNet2DModel):\n",
    "    def forward_latent(\n",
    "        self,\n",
    "        sample: torch.Tensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        class_labels: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[UNet2DOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        The [`UNet2DModel`] forward method.\n",
    "\n",
    "        Args:\n",
    "            sample (`torch.Tensor`):\n",
    "                The noisy input tensor with the following shape `(batch, channel, height, width)`.\n",
    "            timestep (`torch.Tensor` or `float` or `int`): The number of timesteps to denoise an input.\n",
    "            class_labels (`torch.Tensor`, *optional*, defaults to `None`):\n",
    "                Optional class labels for conditioning. Their embeddings will be summed with the timestep embeddings.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~models.unet_2d.UNet2DOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~models.unet_2d.UNet2DOutput`] or `tuple`:\n",
    "                If `return_dict` is True, an [`~models.unet_2d.UNet2DOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is the sample tensor.\n",
    "        \"\"\"\n",
    "        # 0. center input if necessary\n",
    "        if self.config.center_input_sample:\n",
    "            sample = 2 * sample - 1.0\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(sample.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # timesteps does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "        if self.class_embedding is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\"class_labels should be provided when doing class conditioning\")\n",
    "\n",
    "            if self.config.class_embed_type == \"timestep\":\n",
    "                class_labels = self.time_proj(class_labels)\n",
    "\n",
    "            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
    "            emb = emb + class_emb\n",
    "        elif self.class_embedding is None and class_labels is not None:\n",
    "            raise ValueError(\"class_embedding needs to be initialized in order to use class conditioning\")\n",
    "\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = downsample_block(\n",
    "                    hidden_states=sample, temb=emb, skip_sample=skip_sample\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        return sample, down_block_res_samples\n",
    "\n",
    "    def forward_up(\n",
    "        self,\n",
    "        sample: torch.Tensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        class_labels: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "        down_block_res_samples = None,\n",
    "    ) -> Union[UNet2DOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        The [`UNet2DModel`] forward method.\n",
    "\n",
    "        Args:\n",
    "            sample (`torch.Tensor`):\n",
    "                The noisy input tensor with the following shape `(batch, channel, height, width)`.\n",
    "            timestep (`torch.Tensor` or `float` or `int`): The number of timesteps to denoise an input.\n",
    "            class_labels (`torch.Tensor`, *optional*, defaults to `None`):\n",
    "                Optional class labels for conditioning. Their embeddings will be summed with the timestep embeddings.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~models.unet_2d.UNet2DOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~models.unet_2d.UNet2DOutput`] or `tuple`:\n",
    "                If `return_dict` is True, an [`~models.unet_2d.UNet2DOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is the sample tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(sample.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # timesteps does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "        if self.class_embedding is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\"class_labels should be provided when doing class conditioning\")\n",
    "\n",
    "            if self.config.class_embed_type == \"timestep\":\n",
    "                class_labels = self.time_proj(class_labels)\n",
    "\n",
    "            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
    "            emb = emb + class_emb\n",
    "        elif self.class_embedding is None and class_labels is not None:\n",
    "            raise ValueError(\"class_embedding needs to be initialized in order to use class conditioning\")\n",
    "\n",
    "        # 4. mid\n",
    "        sample = self.mid_block(sample, emb)\n",
    "\n",
    "        # 5. up\n",
    "        skip_sample = None\n",
    "        for upsample_block in self.up_blocks:\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            if hasattr(upsample_block, \"skip_conv\"):\n",
    "                sample, skip_sample = upsample_block(sample, res_samples, emb, skip_sample)\n",
    "            else:\n",
    "                sample = upsample_block(sample, res_samples, emb)\n",
    "\n",
    "        # 6. post-process\n",
    "        sample = self.conv_norm_out(sample)\n",
    "        sample = self.conv_act(sample)\n",
    "        sample = self.conv_out(sample)\n",
    "\n",
    "        if skip_sample is not None:\n",
    "            sample += skip_sample\n",
    "\n",
    "        if self.config.time_embedding_type == \"fourier\":\n",
    "            timesteps = timesteps.reshape((sample.shape[0], *([1] * len(sample.shape[1:]))))\n",
    "            sample = sample / timesteps\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sample,)\n",
    "\n",
    "        return UNet2DOutput(sample=sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288a0cc1-fa13-4b2c-ada7-497f0aa765e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device='cpu').manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764e574d-0219-4648-aa9c-5125cabfeab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe25cff-7ac1-4518-bc38-c13a64bd0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50328a1-35c9-4af2-bd5f-ad7aa300c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.utils import denormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ed50bd-1bff-4e03-b8d1-f3484a34d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"/home/simonexc/Documents/DL-2024L-Diffusion/src/run_checkpoints/runs_41dd140bfe6840ae8b6b468c873e150a/epoch_7_step_144000.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d21ec-58d4-4dd5-83b2-57377db0adf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0c036f-5925-49de-bc26-2b85b8558ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "for key in a.keys():\n",
    "    if key[:6] != \"model.\":\n",
    "        continue\n",
    "    b[key.replace(\"model.\", \"\")] = a[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a59a8b0-1b7d-43ff-905a-817abcd92ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Unet(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    block_out_channels=[128, 128, 256, 256, 256, 512],\n",
    "    down_block_types=[\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"],\n",
    "    up_block_types=[\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"],\n",
    "    sample_size=128\n",
    ")\n",
    "model.load_state_dict(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d15d410-279a-42ab-8309-47fe51fcd76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf718205-f308-4bc9-b1f9-276a5d596ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "def inference(images, timesteps=1000):\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    noise_scheduler.set_timesteps(timesteps)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in tqdm(noise_scheduler.timesteps):\n",
    "            predicted_noise = model(images, t, return_dict=False)[0]\n",
    "        \n",
    "            images = noise_scheduler.step(predicted_noise, t, images, generator=generator).prev_sample\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44a9cc32-ca73-449a-b0fe-6e4c68a2d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from dataloaders.utils import denormalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73faa864-374a-4f2a-b7dc-ccd1894998b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "def tensors_to_image(tens):\n",
    "    return Image.fromarray((make_grid(denormalize(full_out), nrow=10) * 255).byte().permute(1,2,0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0addb0ff-217d-4d1f-b1ec-492476cbe233",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = torch.randn(2, 3, 128, 128, generator=generator).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e199df9d-56ad-4eca-adf7-91313a0911a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:19<00:00, 52.18it/s]\n"
     ]
    }
   ],
   "source": [
    "full_out = inference(samp)\n",
    "tensors_to_image(full_out).save(\"test_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6459bfa5-daeb-4739-ab61-75a0b0de4592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:19<00:00, 52.20it/s]\n"
     ]
    }
   ],
   "source": [
    "full_out = inference(samp, 1000)\n",
    "tensors_to_image(full_out).save(\"test_comparison2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97e15b8a-624a-4a4d-b0bb-6843187a5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolate(tensor1, tensor2, t):\n",
    "    return (1 - t) * tensor1 + t * tensor2\n",
    "\n",
    "\n",
    "def inference_with_interpolation(images, timesteps=1000):\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    noise_scheduler.set_timesteps(timesteps)\n",
    "    images = torch.cat([images]*10, dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in tqdm(noise_scheduler.timesteps):\n",
    "            latent, res_samples = model.forward_latent(images, t, return_dict=False)\n",
    "            for i in range(8):\n",
    "                latent[i+1] = linear_interpolate(latent[0], latent[-1], (i+1)/10)\n",
    "            predicted_noise = model.forward_up(latent, t, return_dict=False, down_block_res_samples=res_samples)[0]\n",
    "            \n",
    "            images = noise_scheduler.step(predicted_noise, t, images, generator=generator).prev_sample\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "191cf288-fd0f-4fcc-bc18-a13d22ee6f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:06<00:00, 14.97it/s]\n"
     ]
    }
   ],
   "source": [
    "full_out = inference_with_interpolation(samp[0].unsqueeze(0))\n",
    "tensors_to_image(full_out).save(\"test_comparison_inter.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8987f6b-2f7c-4e7f-a084-ce64baaa8283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
